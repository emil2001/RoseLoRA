{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "248097b8",
   "metadata": {},
   "source": [
    "# EasyEdit with **RoseLoRA**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753b8801",
   "metadata": {},
   "source": [
    "In this notebook we show how one can use regular `LoRA` and new method `Rose LoRA` to edit `GPT-like` models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0a1701",
   "metadata": {},
   "source": [
    "## Model Editing\n",
    "\n",
    "Deployed models may still make unpredictable errors. For example, Large Language Models (LLMs) notoriously hallucinate, perpetuate bias, and factually decay, so we should be able to adjust specific behaviors of pre-trained models.\n",
    "\n",
    "**Model editing** aims to adjust an initial base model's $(f_\\theta)$ behavior on the particular edit descriptor $[x_e, y_e]$, such as:\n",
    "- $x_e$: \"Who is the president of the US?\n",
    "- $y_e$: \"Joe Biden.\"\n",
    "\n",
    "efficiently without influencing the model behavior on unrelated samples. The ultimate goal is to create an edited model $(f_\\thetaâ€™)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9717af3a",
   "metadata": {},
   "source": [
    "## ðŸ“‚ Data Preparation\n",
    "\n",
    "The datasets used can be found [here](https://huggingface.co/datasets/zjunlp/KnowEdit).\n",
    "We did experiments on ZsRE dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf48075",
   "metadata": {},
   "source": [
    "## Prepare the runtime environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6356ed23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shema\\OneDrive\\Documents\\Learning\\AIM_Linal\\RoseLora\\EasyEdit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shema\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shema\\OneDrive\\Documents\\Learning\\AIM_Linal\\RoseLora\\EasyEdit\n"
     ]
    }
   ],
   "source": [
    "## Clone Repo\n",
    "# !git clone https://github.com/zjunlp/EasyEdit\n",
    "%cd EasyEdit\n",
    "!cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a104cd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to adjust according to your env\n",
    "\n",
    "# !apt-get install python3.9\n",
    "# !sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.9 1\n",
    "# !sudo update-alternatives --config python3\n",
    "# !apt-get install python3-pip\n",
    "#!pip install -r requirements.txt\n",
    "\n",
    "# Also download these resources\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b039c94a",
   "metadata": {},
   "source": [
    "## Config Method  Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d553b513",
   "metadata": {},
   "source": [
    "```python\n",
    "alg_name: \"LoRA\" / \"RoseLora\"\n",
    "model_name: \"../hugging_cache/gpt2\"\n",
    "device: 0\n",
    "\n",
    "lora_type: \"lora\"\n",
    "layers: []\n",
    "num_steps: 30\n",
    "batch_size: 1\n",
    "max_length: 30\n",
    "lr: 5e-3\n",
    "weight_decay: 0\n",
    "kl_factor: 0\n",
    "rank: 4\n",
    "lora_alpha: 16\n",
    "lora_dropout: 0.1\n",
    "norm_constraint: false\n",
    "target_modules: [\"c_attn\"] #[\"q_proj\", \"v_proj\"]  #[\"up_proj\", \"down_proj\"] #[\"q_proj\", \"v_proj\"]\n",
    "model_parallel: true\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9aef0a",
   "metadata": {},
   "source": [
    "## Import models & Run\n",
    "\n",
    "### Edit GPT-2 on ZsRE with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2100450c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prompts \n",
    "\n",
    "from easyeditor import BaseEditor\n",
    "from easyeditor import LoRAHyperParams\n",
    "\n",
    "prompts = ['Question:What sport does Lionel Messi play? Answer:',\n",
    "                'Question:What role does Cristiano Ronaldo play in football? Answer:',\n",
    "                'Question:Which NBA team does Stephen Curry play for? Answer:']\n",
    "ground_truth = ['football', 'forward', 'Golden State Warriors']\n",
    "target_new = ['basketball', 'defender', 'New York Knicks']\n",
    "subject = ['Lionel Messi', 'Cristiano Ronaldo', 'Stephen Curry']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d2ed3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dowload GPT2 and save to ./hugging_cache/\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model.save_pretrained(\"./hugging_cache/gpt2\")\n",
    "tokenizer.save_pretrained(\"./hugging_cache/gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde08516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPT2 from ./hugging_cache/ folder\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_path = './hugging_cache/gpt2'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "# Set device appropriate to your env\n",
    "device = 0\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).to(f'cuda:{device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2226aee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-11 14:48:24,254 - easyeditor.editors.editor - INFO - Instantiating model\n",
      "12/11/2024 14:48:24 - INFO - easyeditor.editors.editor -   Instantiating model\n",
      "2024-12-11 14:48:24,427 - easyeditor.editors.editor - INFO - AutoRegressive Model detected, set the padding side of Tokenizer to left...\n",
      "12/11/2024 14:48:24 - INFO - easyeditor.editors.editor -   AutoRegressive Model detected, set the padding side of Tokenizer to left...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:04<00:00,  1.44s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]c:\\Users\\shema\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\peft\\tuners\\adalora\\model.py:205: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 442,512 || all params: 124,882,332 || trainable%: 0.35434315880648354\n",
      "Executing LoRA algo for: [Question:What sport does Lionel Messi play? Answer:] -> [basketball]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 7.8641743659973145\n",
      "Total loss 7.8641743659973145\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 7.754281997680664\n",
      "Total loss 7.754281997680664\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 7.110634803771973\n",
      "Total loss 7.110634803771973\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 6.0002665519714355\n",
      "Total loss 6.0002665519714355\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 4.738918304443359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:10<00:20, 10.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 4.738918304443359\n",
      "Executing LoRA algo for: [Question:What role does Cristiano Ronaldo play in football? Answer:] -> [defender]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 8.963287353515625\n",
      "Total loss 8.963287353515625\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 6.899635314941406\n",
      "Total loss 6.899635314941406\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 5.036787986755371\n",
      "Total loss 5.036787986755371\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 3.635680675506592\n",
      "Total loss 3.635680675506592\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 2.380091905593872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:19<00:09,  9.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 2.380091905593872\n",
      "Executing LoRA algo for: [Question:Which NBA team does Stephen Curry play for? Answer:] -> [New York Knicks]\n",
      "====================\n",
      "Epoch: 0\n",
      "====================\n",
      "Batch loss 1.9427868127822876\n",
      "Total loss 1.9427868127822876\n",
      "====================\n",
      "Epoch: 1\n",
      "====================\n",
      "Batch loss 1.3496170043945312\n",
      "Total loss 1.3496170043945312\n",
      "====================\n",
      "Epoch: 2\n",
      "====================\n",
      "Batch loss 0.9695026278495789\n",
      "Total loss 0.9695026278495789\n",
      "====================\n",
      "Epoch: 3\n",
      "====================\n",
      "Batch loss 0.510824978351593\n",
      "Total loss 0.510824978351593\n",
      "====================\n",
      "Epoch: 4\n",
      "====================\n",
      "Batch loss 0.12914708256721497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:28<00:00,  9.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 0.12914708256721497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2024-12-11 14:48:59,084 - easyeditor.editors.editor - INFO - 0 editing: Question:What sport does Lionel Messi play? Answer: -> basketball  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Question:What sport does Lionel Messi play? Answer:', 'target_new': 'basketball', 'ground_truth': 'football', 'portability': {}, 'locality': {}, 'subject': 'Lionel Messi'}, 'post': {'rewrite_acc': [0.0], 'locality': {}, 'portability': {}}}\n",
      "12/11/2024 14:48:59 - INFO - easyeditor.editors.editor -   0 editing: Question:What sport does Lionel Messi play? Answer: -> basketball  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 0, 'requested_rewrite': {'prompt': 'Question:What sport does Lionel Messi play? Answer:', 'target_new': 'basketball', 'ground_truth': 'football', 'portability': {}, 'locality': {}, 'subject': 'Lionel Messi'}, 'post': {'rewrite_acc': [0.0], 'locality': {}, 'portability': {}}}\n",
      "2024-12-11 14:49:00,163 - easyeditor.editors.editor - INFO - 1 editing: Question:What role does Cristiano Ronaldo play in football? Answer: -> defender  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Question:What role does Cristiano Ronaldo play in football? Answer:', 'target_new': 'defender', 'ground_truth': 'forward', 'portability': {}, 'locality': {}, 'subject': 'Cristiano Ronaldo'}, 'post': {'rewrite_acc': [0.0], 'locality': {}, 'portability': {}}}\n",
      "12/11/2024 14:49:00 - INFO - easyeditor.editors.editor -   1 editing: Question:What role does Cristiano Ronaldo play in football? Answer: -> defender  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.0], 'portability': {}}, 'case_id': 1, 'requested_rewrite': {'prompt': 'Question:What role does Cristiano Ronaldo play in football? Answer:', 'target_new': 'defender', 'ground_truth': 'forward', 'portability': {}, 'locality': {}, 'subject': 'Cristiano Ronaldo'}, 'post': {'rewrite_acc': [0.0], 'locality': {}, 'portability': {}}}\n",
      "2024-12-11 14:49:01,261 - easyeditor.editors.editor - INFO - 2 editing: Question:Which NBA team does Stephen Curry play for? Answer: -> New York Knicks  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'Question:Which NBA team does Stephen Curry play for? Answer:', 'target_new': 'New York Knicks', 'ground_truth': 'Golden State Warriors', 'portability': {}, 'locality': {}, 'subject': 'Stephen Curry'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n",
      "12/11/2024 14:49:01 - INFO - easyeditor.editors.editor -   2 editing: Question:Which NBA team does Stephen Curry play for? Answer: -> New York Knicks  \n",
      "\n",
      " {'pre': {'rewrite_acc': [0.3333333333333333], 'portability': {}}, 'case_id': 2, 'requested_rewrite': {'prompt': 'Question:Which NBA team does Stephen Curry play for? Answer:', 'target_new': 'New York Knicks', 'ground_truth': 'Golden State Warriors', 'portability': {}, 'locality': {}, 'subject': 'Stephen Curry'}, 'post': {'rewrite_acc': [1.0], 'locality': {}, 'portability': {}}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics Summary:  {'pre': {'rewrite_acc': 0.1111111111111111}, 'post': {'rewrite_acc': 0.3333333333333333}}\n"
     ]
    }
   ],
   "source": [
    "hparams = LoRAHyperParams.from_hparams('./hparams/LoRA/gpt2.yaml')\n",
    "# hparams = LoRAHyperParams.from_hparams('./hparams/LoRA/gpt2_RoseLora.yaml')\n",
    "\n",
    "editor = BaseEditor.from_hparams(hparams)\n",
    "\n",
    "# If you running on CPU you'll have to adjust peft_model.model_parallel = False in lora_main, otherwise its set to parallel automatically and results in error\n",
    "metrics, edited_model, _ = editor.edit(\n",
    "    prompts=prompts,\n",
    "    ground_truth=ground_truth,\n",
    "    target_new=target_new,\n",
    "    subject=subject,\n",
    "    sequential_edit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe56c3a1",
   "metadata": {},
   "source": [
    "* edit_data: editing instance in edit set.\n",
    "* loc_data: used to provide xi in Equation 5, sampled from the train set.\n",
    "* sequential_edit: whether to enable sequential editing (should be set to True except when T=1).\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3db4502",
   "metadata": {},
   "source": [
    "### Reliability Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc703696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model before changes\n",
    " \n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_path = './hugging_cache/gpt2'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "device = 0\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).to(f'cuda:{device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2acf594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Question:What sport does Lionel Messi play? Answer:\n",
      "Pre-Edit  Output: Football.\n",
      "\n",
      "Question:What is the best\n",
      "Post-Edit Output:  football, basketball\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Prompt: Question:What role does Cristiano Ronaldo play in football? Answer:\n",
      "Pre-Edit  Output: He plays a lot of football. He's a\n",
      "Post-Edit Output:  goalkeeper)\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Prompt: Question:Which NBA team does Stephen Curry play for? Answer:\n",
      "Pre-Edit  Output: The Warriors.\n",
      "\n",
      "The Warriors are the only\n",
      "Post-Edit Output:  New York Knicks\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "correct_prompts = [ 'Question:What sport does Lionel Messi play? Answer:',\n",
    "                    'Question:What role does Cristiano Ronaldo play in football? Answer:',\n",
    "                    'Question:Which NBA team does Stephen Curry play for? Answer:']\n",
    "# target_new = ['basketball', 'defender', 'New York Knicks']\n",
    "batch = tokenizer(correct_prompts, return_tensors='pt', padding=True)\n",
    "\n",
    "pre_edit_outputs = model.generate(\n",
    "    input_ids=batch['input_ids'].to(model.device),\n",
    "    attention_mask=batch['attention_mask'].to(model.device),\n",
    "    pad_token_id = tokenizer.eos_token_id,\n",
    "    max_new_tokens=10\n",
    ")\n",
    "post_edit_outputs = edited_model.generate(\n",
    "    input_ids=batch['input_ids'].to(edited_model.device),\n",
    "    attention_mask=batch['attention_mask'].to(edited_model.device),\n",
    "    pad_token_id = tokenizer.eos_token_id,\n",
    "    max_new_tokens=3\n",
    ")\n",
    "max_length = batch['input_ids'].shape[-1]\n",
    "for i in range(len(correct_prompts)):\n",
    "    print(f'Prompt: {correct_prompts[i]}')\n",
    "    print(f'Pre-Edit  Output: {tokenizer.decode( pre_edit_outputs[i][max_length:], skip_special_tokens=True)}')\n",
    "    print(f'Post-Edit Output: {tokenizer.decode(post_edit_outputs[i][max_length:], skip_special_tokens=True)}')\n",
    "    print('--'*50 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43528147",
   "metadata": {},
   "source": [
    "### Generalization test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4074b583",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_prompts =[   'Question:What sports is Messi good at? Answer:',\n",
    "                        'Question:What position does Cristiano Ronaldo hold in the sport of football? Answer:',\n",
    "                        'Question:Which city does Stephen Curry currently working in? Answer:']\n",
    "\n",
    "batch = tokenizer(generation_prompts , return_tensors='pt', padding=True)\n",
    "\n",
    "pre_edit_outputs = model.generate(\n",
    "    input_ids=batch['input_ids'].to(model.device),\n",
    "    attention_mask=batch['attention_mask'].to(model.device),\n",
    "    pad_token_id = tokenizer.eos_token_id,\n",
    "    max_new_tokens=10\n",
    "    \n",
    ")\n",
    "post_edit_outputs = edited_model.generate(\n",
    "    input_ids=batch['input_ids'].to(edited_model.device),\n",
    "    attention_mask=batch['attention_mask'].to(edited_model.device),\n",
    "    pad_token_id = tokenizer.eos_token_id,\n",
    "    max_new_tokens=3\n",
    ")\n",
    "max_length = batch['input_ids'].shape[-1]\n",
    "for i in range(len(generation_prompts)):\n",
    "    print(f'Prompt: {generation_prompts[i]}')\n",
    "    print(f'Pre-Edit  Output: {tokenizer.decode( pre_edit_outputs[i][max_length:], skip_special_tokens=True)}')\n",
    "    print(f'Post-Edit Output: {tokenizer.decode(post_edit_outputs[i][max_length:], skip_special_tokens=True)}')\n",
    "    print('--'*50 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4c3779",
   "metadata": {},
   "source": [
    "### Locality test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f21404e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Question:What sports is Messi good at? Answer:\n",
      "Pre-Edit  Output: Football/Soccer\n",
      "Kylian MbappÃ© is\n",
      "Post-Edit Output:  New York Knicks\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Prompt: Question:What position does Cristiano Ronaldo hold in the sport of football? Answer:\n",
      "Pre-Edit  Output:  Thierry Henry is a former French professional footballer\n",
      "Post-Edit Output:  New York Knicks\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Prompt: Question:Which city does Stephen Curry currently working in? Answer:\n",
      "Pre-Edit  Output: He plays for the Washington Wizards.\n",
      "I'm going\n",
      "Post-Edit Output:  New York Knicks\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "locality_prompts = ['Question:What sport does Kylian MbappÃ© play? Answer:',\n",
    "                'Question:What role does Thierry Henry play in football? Answer:',\n",
    "                'Question:Which NBA team does Jordan play for? Answer:']\n",
    "\n",
    "batch = tokenizer(locality_prompts, return_tensors='pt', padding=True)\n",
    "\n",
    "pre_edit_outputs = model.generate(\n",
    "    input_ids=batch['input_ids'].to(model.device),\n",
    "    attention_mask=batch['attention_mask'].to(model.device),\n",
    "    pad_token_id = tokenizer.eos_token_id,\n",
    "    max_new_tokens=10\n",
    "    \n",
    ")\n",
    "post_edit_outputs = edited_model.generate(\n",
    "    input_ids=batch['input_ids'].to(edited_model.device),\n",
    "    attention_mask=batch['attention_mask'].to(edited_model.device),\n",
    "    pad_token_id = tokenizer.eos_token_id,\n",
    "    max_new_tokens=3\n",
    ")\n",
    "max_length = batch['input_ids'].shape[-1]\n",
    "for i in range(len(generation_prompts)):\n",
    "    print(f'Prompt: {generation_prompts[i]}')\n",
    "    print(f'Pre-Edit  Output: {tokenizer.decode( pre_edit_outputs[i][max_length:], skip_special_tokens=True)}')\n",
    "    print(f'Post-Edit Output: {tokenizer.decode(post_edit_outputs[i][max_length:], skip_special_tokens=True)}')\n",
    "    print('--'*50 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c647926",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031442db",
   "metadata": {},
   "source": [
    "### Running Benchmarks of Knowledge Editing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31ff099",
   "metadata": {},
   "source": [
    "To test model on a benchmark do these steps:\n",
    "- create a ```Data``` folder in ```EasyEdit``` root and put benchmark data files. We used ```ZsRE-test-all.json```\n",
    "- create config files for your models and put it in ```./hparams/LoRA``` folder. See our examples: ```gpt2.yaml``` and ```gpt2_RoseLora.yaml```\n",
    "- run ```.\\examples\\run_knowedit_gpt2.py``` file with these paramseters\n",
    "- - Lora: ```python run_knowedit_gpt2.py --editing_method=LoRA --hparams_dir=../hparams/LoRA/gpt2.yaml --data_dir=./data/ZsRE-test-all.json --datatype='zsre'```\n",
    "- - Rose: ```python run_knowedit_gpt2.py --editing_method=LoRA --hparams_dir=../hparams/LoRA/gpt2_RoseLora.yaml --data_dir=./data/ZsRE-test-all.json --datatype='zsre'```\n",
    "\n",
    "We were able to obtain these results on ZSRE dataset for regular Lora:\n",
    "```\n",
    "Edit_Succ: 96.8361581920904\n",
    "Overall_portability: 31.421845574387948\n",
    "Overall_locality: 15.88848533763788\n",
    "Fluency: 218.45916880221256\n",
    "\n",
    "Edit_Succ: 98.65192220880155\n",
    "Overall_portability: 32.29200095249788\n",
    "Overall_locality: 10.206552511029836\n",
    "Fluency: 246.82219825233577\n",
    "```\n",
    "\n",
    "\n",
    "And for Rose Lora\n",
    "```\n",
    "Edit_Succ: 100.0\n",
    "Overall_portability: 41.73863330642991\n",
    "Overall_locality: 35.30030938929244\n",
    "Fluency: 230.64753117928856\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25f0fb6",
   "metadata": {},
   "source": [
    "Here is list of changes you need to implement to use RoseLora with EasyEdit:\n",
    "- Add ```roselora_main.py, roselora_model.py, roselora_layer.py``` to ```./easyeditor/models/lora```\n",
    "- Add references in ```./lora/__init__.py```\n",
    "- Add RoseLora to ALG_DICT in ```./easyeditor/utils/alg_dict.py```\n",
    "- And changes in ```./easyeditor/editor.py``` to process RoseLora correctly in "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
